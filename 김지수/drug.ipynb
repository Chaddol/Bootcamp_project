{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "이 데이터는 다른 코드에서 찾은 혈압약 데이터\n",
    "'''\n",
    "bp_drug_code = ['K-019699', 'K-019700', 'K-019861', 'K-019867', 'K-020378', 'K-020379',\n",
    "                'K-020401', 'K-020834', 'K-020877', 'K-023252', 'K-023357', 'K-023779',\n",
    "                'K-023829', 'K-023838', 'K-024162', 'K-025000', 'K-025033', 'K-025469',\n",
    "                'K-025470', 'K-025728', 'K-025886', 'K-026694', 'K-027187', 'K-027188',\n",
    "                'K-027733', 'K-027735', 'K-027840', 'K-028010', 'K-028081', 'K-028129',\n",
    "                'K-028130', 'K-028360', 'K-028588', 'K-028813', 'K-030005', 'K-030063',\n",
    "                'K-030064', 'K-030690', 'K-031543', 'K-031545', 'K-035291', 'K-036170',\n",
    "                'K-036171', 'K-036172', 'K-037086', 'K-039150', 'K-040099', 'K-040639',\n",
    "                'K-040671', 'K-040996', 'K-041947', 'K-041948', 'K-041949', 'K-042118',\n",
    "                'K-042119', 'K-042120', 'K-051326', 'K-000798', 'K-000869', 'K-001095',\n",
    "                'K-001579', 'K-001899', 'K-003414', 'K-003709', 'K-005856', 'K-001715',\n",
    "                'K-002159', 'K-003049', 'K-003157', 'K-006854', 'K-006855', 'K-007059',\n",
    "                'K-007402', 'K-011299', 'K-003708', 'K-004478', 'K-013938', 'K-014088',\n",
    "                'K-014275', 'K-015266', 'K-016218', 'K-019438', 'K-019796', 'K-007060',\n",
    "                'K-020451', 'K-020558', 'K-020878', 'K-022330', 'K-022331', 'K-023631',\n",
    "                'K-024489', 'K-025078', 'K-026813', 'K-027652', 'K-027653', 'K-027776',\n",
    "                'K-027777', 'K-028128', 'K-028359', 'K-028602', 'K-028683', 'K-028684',\n",
    "                'K-028814', 'K-029315', 'K-030003', 'K-030062', 'K-030145', 'K-030244',\n",
    "                'K-031168', 'K-031544', 'K-011252', 'K-011253', 'K-011647', 'K-033398',\n",
    "                'K-034251', 'K-034502', 'K-034503', 'K-034812', 'K-039146', 'K-039147',\n",
    "                'K-015190', 'K-016550', 'K-016912', 'K-017506', 'K-017511', 'K-017525', 'K-018119']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['K-038884',\n",
       " 'K-038890',\n",
       " 'K-038896',\n",
       " 'K-038910',\n",
       " 'K-038912',\n",
       " 'K-038913',\n",
       " 'K-038914',\n",
       " 'K-038927',\n",
       " 'K-038929',\n",
       " 'K-038954',\n",
       " 'K-038958',\n",
       " 'K-038959',\n",
       " 'K-038962',\n",
       " 'K-038967',\n",
       " 'K-038970',\n",
       " 'K-039021',\n",
       " 'K-039036',\n",
       " 'K-039047',\n",
       " 'K-039104',\n",
       " 'K-039108',\n",
       " 'K-039123',\n",
       " 'K-039136',\n",
       " 'K-039146',\n",
       " 'K-039147']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mother_dir = 'images/'\n",
    "folder_list = os.listdir(mother_dir)\n",
    "folder_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9612\n",
      "images//K-038884\\K-038884_0_0_0_0_75_000_200.png\n",
      "images//K-038884\\K-038884_0_0_0_0_75_020_200.png\n",
      "images//K-038884\\K-038884_0_0_0_0_75_040_200.png\n",
      "images//K-038884\\K-038884_0_0_0_0_75_060_200.png\n",
      "images//K-038884\\K-038884_0_0_0_0_75_080_200.png\n",
      "images//K-038884\\K-038884_0_0_0_0_75_100_200.png\n",
      "images//K-038884\\K-038884_0_0_0_0_75_120_200.png\n",
      "images//K-038884\\K-038884_0_0_0_0_75_140_200.png\n",
      "images//K-038884\\K-038884_0_0_0_0_75_160_200.png\n",
      "images//K-038884\\K-038884_0_0_0_0_75_180_200.png\n"
     ]
    }
   ],
   "source": [
    "img_file_names = []\n",
    "for x in folder_list:\n",
    "  img_file_names += glob.glob(mother_dir + '/' + x + '/*.*')\n",
    "\n",
    "print(len(img_file_names))\n",
    "for i in range(10):\n",
    "  print(img_file_names[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'K-038884': 0,\n",
       " 'K-038890': 1,\n",
       " 'K-038896': 2,\n",
       " 'K-038910': 3,\n",
       " 'K-038912': 4,\n",
       " 'K-038913': 5,\n",
       " 'K-038914': 6,\n",
       " 'K-038927': 7,\n",
       " 'K-038929': 8,\n",
       " 'K-038954': 9,\n",
       " 'K-038958': 10,\n",
       " 'K-038959': 11,\n",
       " 'K-038962': 12,\n",
       " 'K-038967': 13,\n",
       " 'K-038970': 14,\n",
       " 'K-039021': 15,\n",
       " 'K-039036': 16,\n",
       " 'K-039047': 17,\n",
       " 'K-039104': 18,\n",
       " 'K-039108': 19,\n",
       " 'K-039123': 20,\n",
       " 'K-039136': 21,\n",
       " 'K-039146': 22,\n",
       " 'K-039147': 23}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls = {}\n",
    "for idx,folder in enumerate(folder_list):\n",
    "  cls[folder] = idx\n",
    "\n",
    "cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images//K-038929\\\\K-038929_0_0_0_1_75_320_200.png',\n",
       " 'images//K-038896\\\\K-038896_0_1_0_1_90_100_200.png',\n",
       " 'images//K-039047\\\\K-039047_0_2_1_0_90_080_200.png',\n",
       " 'images//K-038910\\\\K-038910_0_0_0_0_70_200_200.png',\n",
       " 'images//K-039147\\\\K-039147_0_0_1_0_75_120_200.png',\n",
       " 'images//K-039147\\\\K-039147_0_1_1_2_70_300_200.png',\n",
       " 'images//K-039147\\\\K-039147_0_2_0_1_90_120_200.png',\n",
       " 'images//K-038929\\\\K-038929_0_0_0_2_70_100_200.png',\n",
       " 'images//K-039047\\\\K-039047_0_2_0_0_90_180_200.png',\n",
       " 'images//K-039136\\\\K-039136_0_1_0_2_75_160_200.png']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_names, x_test_names = train_test_split(img_file_names, test_size=0.2, random_state=1)\n",
    "x_train_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images//K-038929\\K-038929_0_0_0_1_75_320_200.png\n",
      "images//K-038910\\K-038910_0_0_0_0_75_260_200.png\n",
      "images//K-038958\\K-038958_0_2_0_0_75_240_200.png\n",
      "images//K-038967\\K-038967_0_2_1_1_70_220_200.png\n",
      "images//K-038954\\K-038954_0_2_1_1_70_100_200.png\n",
      "images//K-039136\\K-039136_0_1_0_1_60_300_200.png\n",
      "images//K-038970\\K-038970_0_2_1_0_90_180_200.png\n",
      "images//K-038954\\K-038954_0_2_1_0_75_280_200.png\n",
      "images//K-039147\\K-039147_0_0_1_1_60_260_200.png\n",
      "images//K-039147\\K-039147_0_1_0_1_60_340_200.png\n",
      "images//K-039147\\K-039147_0_2_1_1_75_020_200.png\n",
      "images//K-038890\\K-038890_0_0_1_1_75_320_200.png\n",
      "images//K-038890\\K-038890_0_0_0_1_90_300_200.png\n",
      "images//K-039146\\K-039146_0_2_0_2_90_060_200.png\n",
      "images//K-039108\\K-039108_0_0_0_0_75_000_200.png\n",
      "images//K-038914\\K-038914_0_0_1_2_90_020_200.png\n",
      "images//K-039147\\K-039147_0_0_0_0_75_260_200.png\n",
      "images//K-038912\\K-038912_0_0_0_1_90_020_200.png\n",
      "images//K-039146\\K-039146_0_2_0_2_90_300_200.png\n",
      "images//K-039146\\K-039146_0_2_1_2_75_060_200.png\n",
      "images//K-038914\\K-038914_0_0_1_1_90_260_200.png\n",
      "images//K-038958\\K-038958_0_2_0_1_70_020_200.png\n",
      "images//K-039047\\K-039047_0_2_0_2_90_180_200.png\n",
      "images//K-039036\\K-039036_0_1_0_2_90_340_200.png\n",
      "images//K-039136\\K-039136_0_1_1_1_70_100_200.png\n",
      "images//K-038929\\K-038929_0_0_1_1_70_140_200.png\n",
      "images//K-039146\\K-039146_0_0_0_1_60_080_200.png\n",
      "images//K-038954\\K-038954_0_2_1_0_75_180_200.png\n",
      "images//K-038958\\K-038958_0_2_1_0_70_340_200.png\n",
      "images//K-039147\\K-039147_0_1_0_1_70_060_200.png\n",
      "images//K-039136\\K-039136_0_0_0_0_70_220_200.png\n",
      "images//K-038914\\K-038914_0_0_0_1_75_180_200.png\n",
      "images//K-038970\\K-038970_0_2_0_1_90_240_200.png\n",
      "images//K-039147\\K-039147_0_1_0_0_70_260_200.png\n",
      "images//K-039036\\K-039036_0_1_0_1_75_260_200.png\n",
      "images//K-039147\\K-039147_0_2_1_2_75_100_200.png\n",
      "images//K-038912\\K-038912_0_0_0_1_75_240_200.png\n",
      "images//K-039147\\K-039147_0_1_1_1_70_300_200.png\n",
      "images//K-038962\\K-038962_0_2_1_1_90_300_200.png\n",
      "images//K-038910\\K-038910_0_0_0_0_75_220_200.png\n",
      "images//K-038913\\K-038913_0_0_0_0_75_140_200.png\n",
      "images//K-038967\\K-038967_0_2_1_1_75_140_200.png\n",
      "images//K-039147\\K-039147_0_2_1_1_70_100_200.png\n",
      "images//K-039146\\K-039146_0_0_1_0_60_320_200.png\n",
      "images//K-039147\\K-039147_0_1_1_1_75_100_200.png\n",
      "images//K-039147\\K-039147_0_1_0_2_60_280_200.png\n",
      "images//K-038896\\K-038896_0_1_1_0_70_020_200.png\n",
      "images//K-039108\\K-039108_0_0_0_2_90_180_200.png\n",
      "images//K-038913\\K-038913_0_0_0_0_90_260_200.png\n",
      "images//K-039104\\K-039104_0_0_0_0_75_220_200.png\n",
      "images//K-038914\\K-038914_0_0_1_0_75_320_200.png\n",
      "images//K-039047\\K-039047_0_2_1_2_70_000_200.png\n",
      "images//K-039136\\K-039136_0_0_0_2_70_140_200.png\n",
      "images//K-039108\\K-039108_0_0_0_1_90_320_200.png\n",
      "images//K-038896\\K-038896_0_1_0_0_75_260_200.png\n",
      "images//K-038910\\K-038910_0_0_0_1_75_000_200.png\n",
      "images//K-038959\\K-038959_0_0_0_0_90_120_200.png\n",
      "images//K-038962\\K-038962_0_2_1_2_90_240_200.png\n",
      "images//K-039147\\K-039147_0_1_0_0_90_200_200.png\n",
      "images//K-038912\\K-038912_0_0_1_0_75_220_200.png\n",
      "images//K-039146\\K-039146_0_2_0_0_70_220_200.png\n",
      "images//K-038962\\K-038962_0_2_1_1_75_040_200.png\n",
      "images//K-039136\\K-039136_0_2_0_1_60_320_200.png\n",
      "images//K-038954\\K-038954_0_2_1_1_90_060_200.png\n",
      "images//K-039136\\K-039136_0_1_1_1_60_100_200.png\n",
      "images//K-039047\\K-039047_0_2_1_2_90_280_200.png\n",
      "images//K-039146\\K-039146_0_0_1_0_60_260_200.png\n",
      "images//K-039146\\K-039146_0_2_0_0_60_240_200.png\n",
      "images//K-039136\\K-039136_0_1_1_1_75_260_200.png\n",
      "images//K-038912\\K-038912_0_0_1_1_90_320_200.png\n",
      "images//K-039147\\K-039147_0_0_1_1_90_000_200.png\n",
      "images//K-038967\\K-038967_0_2_0_1_75_020_200.png\n",
      "images//K-039146\\K-039146_0_2_0_2_70_180_200.png\n",
      "images//K-038967\\K-038967_0_2_1_1_90_080_200.png\n",
      "images//K-039146\\K-039146_0_0_1_0_75_160_200.png\n",
      "images//K-038967\\K-038967_0_2_1_1_90_220_200.png\n",
      "images//K-038910\\K-038910_0_0_1_2_90_100_200.png\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for idx, x in enumerate(x_train_names):\n",
    "  if idx % 100 == 0:\n",
    "    print(x)\n",
    "  img = cv2.cvtColor(cv2.imread(x, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "  id = x[s:s+8]\n",
    "  temp_cls = cls[id]\n",
    "  y_train.append(temp_cls)\n",
    "  x_train.append(img)\n",
    "\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.41568627, 0.28627451, 0.21960784],\n",
       "         [0.41568627, 0.2745098 , 0.23137255],\n",
       "         [0.41568627, 0.2745098 , 0.21960784],\n",
       "         ...,\n",
       "         [0.39607843, 0.27843137, 0.22352941],\n",
       "         [0.39607843, 0.28235294, 0.22745098],\n",
       "         [0.38823529, 0.28627451, 0.21960784]],\n",
       "\n",
       "        [[0.41568627, 0.28627451, 0.21568627],\n",
       "         [0.41568627, 0.2745098 , 0.21960784],\n",
       "         [0.41568627, 0.2745098 , 0.21960784],\n",
       "         ...,\n",
       "         [0.40784314, 0.27058824, 0.21176471],\n",
       "         [0.41176471, 0.26666667, 0.21176471],\n",
       "         [0.39607843, 0.27843137, 0.21960784]],\n",
       "\n",
       "        [[0.41568627, 0.29411765, 0.21960784],\n",
       "         [0.42352941, 0.29019608, 0.21960784],\n",
       "         [0.42745098, 0.28235294, 0.21176471],\n",
       "         ...,\n",
       "         [0.39607843, 0.28235294, 0.22745098],\n",
       "         [0.39607843, 0.27843137, 0.21960784],\n",
       "         [0.40392157, 0.2745098 , 0.21176471]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.43529412, 0.29411765, 0.21960784],\n",
       "         [0.42352941, 0.29019608, 0.22745098],\n",
       "         [0.42352941, 0.29411765, 0.22745098],\n",
       "         ...,\n",
       "         [0.40784314, 0.27843137, 0.21568627],\n",
       "         [0.40784314, 0.28235294, 0.21568627],\n",
       "         [0.40784314, 0.27843137, 0.21176471]],\n",
       "\n",
       "        [[0.42352941, 0.29803922, 0.21176471],\n",
       "         [0.42352941, 0.29019608, 0.23137255],\n",
       "         [0.42352941, 0.29803922, 0.22745098],\n",
       "         ...,\n",
       "         [0.40784314, 0.2745098 , 0.21960784],\n",
       "         [0.40784314, 0.27843137, 0.21176471],\n",
       "         [0.41568627, 0.2745098 , 0.23137255]],\n",
       "\n",
       "        [[0.42352941, 0.28627451, 0.23137255],\n",
       "         [0.43137255, 0.28627451, 0.21568627],\n",
       "         [0.42352941, 0.29803922, 0.22745098],\n",
       "         ...,\n",
       "         [0.41568627, 0.28235294, 0.21960784],\n",
       "         [0.41568627, 0.26666667, 0.21960784],\n",
       "         [0.41568627, 0.2745098 , 0.21960784]]],\n",
       "\n",
       "\n",
       "       [[[0.56078431, 0.51764706, 0.41568627],\n",
       "         [0.56470588, 0.5254902 , 0.4       ],\n",
       "         [0.56862745, 0.52941176, 0.42745098],\n",
       "         ...,\n",
       "         [0.55294118, 0.50980392, 0.41960784],\n",
       "         [0.54509804, 0.51372549, 0.42352941],\n",
       "         [0.52941176, 0.50980392, 0.41960784]],\n",
       "\n",
       "        [[0.58039216, 0.5254902 , 0.40784314],\n",
       "         [0.56470588, 0.51372549, 0.42745098],\n",
       "         [0.56078431, 0.5254902 , 0.43137255],\n",
       "         ...,\n",
       "         [0.54509804, 0.50980392, 0.41568627],\n",
       "         [0.54901961, 0.50196078, 0.41568627],\n",
       "         [0.55294118, 0.50588235, 0.41960784]],\n",
       "\n",
       "        [[0.55686275, 0.5254902 , 0.41176471],\n",
       "         [0.57254902, 0.51764706, 0.41176471],\n",
       "         [0.57254902, 0.51372549, 0.41960784],\n",
       "         ...,\n",
       "         [0.54509804, 0.51372549, 0.41960784],\n",
       "         [0.5372549 , 0.51372549, 0.41568627],\n",
       "         [0.55294118, 0.50588235, 0.41176471]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.52156863, 0.48235294, 0.39607843],\n",
       "         [0.5372549 , 0.49019608, 0.4       ],\n",
       "         [0.53333333, 0.48235294, 0.4       ],\n",
       "         ...,\n",
       "         [0.51372549, 0.48235294, 0.38431373],\n",
       "         [0.50980392, 0.47843137, 0.38823529],\n",
       "         [0.52156863, 0.48627451, 0.38431373]],\n",
       "\n",
       "        [[0.53333333, 0.49019608, 0.40392157],\n",
       "         [0.50980392, 0.48235294, 0.39607843],\n",
       "         [0.51764706, 0.48235294, 0.38823529],\n",
       "         ...,\n",
       "         [0.50588235, 0.4745098 , 0.38039216],\n",
       "         [0.50980392, 0.48235294, 0.38431373],\n",
       "         [0.51764706, 0.48235294, 0.38431373]],\n",
       "\n",
       "        [[0.51372549, 0.47843137, 0.39607843],\n",
       "         [0.51764706, 0.48235294, 0.4       ],\n",
       "         [0.52156863, 0.47843137, 0.40784314],\n",
       "         ...,\n",
       "         [0.51764706, 0.48235294, 0.39215686],\n",
       "         [0.51764706, 0.47843137, 0.38823529],\n",
       "         [0.52156863, 0.48235294, 0.38823529]]],\n",
       "\n",
       "\n",
       "       [[[0.78431373, 0.59607843, 0.31764706],\n",
       "         [0.78823529, 0.58823529, 0.3254902 ],\n",
       "         [0.79607843, 0.58823529, 0.31372549],\n",
       "         ...,\n",
       "         [0.78823529, 0.57647059, 0.31372549],\n",
       "         [0.77254902, 0.57647059, 0.31372549],\n",
       "         [0.77647059, 0.57647059, 0.31764706]],\n",
       "\n",
       "        [[0.78431373, 0.58823529, 0.31764706],\n",
       "         [0.78823529, 0.58431373, 0.31764706],\n",
       "         [0.80392157, 0.58431373, 0.32156863],\n",
       "         ...,\n",
       "         [0.75686275, 0.58039216, 0.32156863],\n",
       "         [0.77647059, 0.57647059, 0.30980392],\n",
       "         [0.77647059, 0.57254902, 0.32156863]],\n",
       "\n",
       "        [[0.79215686, 0.58823529, 0.30980392],\n",
       "         [0.78431373, 0.58431373, 0.32156863],\n",
       "         [0.76470588, 0.58823529, 0.32156863],\n",
       "         ...,\n",
       "         [0.78431373, 0.57647059, 0.31372549],\n",
       "         [0.78431373, 0.58039216, 0.31372549],\n",
       "         [0.77647059, 0.58039216, 0.30980392]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.74117647, 0.56078431, 0.30980392],\n",
       "         [0.75294118, 0.55686275, 0.30196078],\n",
       "         [0.73333333, 0.55686275, 0.30588235],\n",
       "         ...,\n",
       "         [0.76470588, 0.55686275, 0.30980392],\n",
       "         [0.74117647, 0.55686275, 0.30588235],\n",
       "         [0.7372549 , 0.56078431, 0.30196078]],\n",
       "\n",
       "        [[0.75686275, 0.56078431, 0.30588235],\n",
       "         [0.75686275, 0.55294118, 0.30980392],\n",
       "         [0.74509804, 0.56078431, 0.30588235],\n",
       "         ...,\n",
       "         [0.7372549 , 0.56078431, 0.30588235],\n",
       "         [0.73333333, 0.55686275, 0.30980392],\n",
       "         [0.7372549 , 0.55686275, 0.30588235]],\n",
       "\n",
       "        [[0.74901961, 0.56078431, 0.30980392],\n",
       "         [0.74509804, 0.55686275, 0.29411765],\n",
       "         [0.74117647, 0.55686275, 0.30588235],\n",
       "         ...,\n",
       "         [0.73333333, 0.56078431, 0.30588235],\n",
       "         [0.75294118, 0.55686275, 0.31372549],\n",
       "         [0.7372549 , 0.54901961, 0.30980392]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.36078431, 0.2627451 , 0.30980392],\n",
       "         [0.35686275, 0.2745098 , 0.3254902 ],\n",
       "         [0.35294118, 0.26666667, 0.3254902 ],\n",
       "         ...,\n",
       "         [0.35294118, 0.27058824, 0.32941176],\n",
       "         [0.35686275, 0.2745098 , 0.3254902 ],\n",
       "         [0.34901961, 0.27843137, 0.31764706]],\n",
       "\n",
       "        [[0.35686275, 0.27058824, 0.32156863],\n",
       "         [0.34509804, 0.28235294, 0.32941176],\n",
       "         [0.36078431, 0.27058824, 0.32156863],\n",
       "         ...,\n",
       "         [0.34509804, 0.27058824, 0.3372549 ],\n",
       "         [0.36470588, 0.26666667, 0.31764706],\n",
       "         [0.35294118, 0.27843137, 0.3254902 ]],\n",
       "\n",
       "        [[0.36078431, 0.25490196, 0.32156863],\n",
       "         [0.34509804, 0.27843137, 0.34117647],\n",
       "         [0.35686275, 0.2627451 , 0.33333333],\n",
       "         ...,\n",
       "         [0.34509804, 0.26666667, 0.33333333],\n",
       "         [0.35294118, 0.25882353, 0.32941176],\n",
       "         [0.36078431, 0.2745098 , 0.34117647]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.3372549 , 0.23529412, 0.31372549],\n",
       "         [0.3254902 , 0.24313725, 0.31372549],\n",
       "         [0.32941176, 0.23529412, 0.30588235],\n",
       "         ...,\n",
       "         [0.32156863, 0.22352941, 0.29411765],\n",
       "         [0.32156863, 0.21568627, 0.30196078],\n",
       "         [0.31372549, 0.23137255, 0.30980392]],\n",
       "\n",
       "        [[0.3372549 , 0.25098039, 0.31372549],\n",
       "         [0.3254902 , 0.23529412, 0.32156863],\n",
       "         [0.32941176, 0.23921569, 0.30980392],\n",
       "         ...,\n",
       "         [0.32156863, 0.21960784, 0.28235294],\n",
       "         [0.32156863, 0.21960784, 0.30196078],\n",
       "         [0.30980392, 0.23921569, 0.30588235]],\n",
       "\n",
       "        [[0.3254902 , 0.23529412, 0.32156863],\n",
       "         [0.3372549 , 0.23529412, 0.31372549],\n",
       "         [0.3254902 , 0.23921569, 0.31372549],\n",
       "         ...,\n",
       "         [0.32156863, 0.23137255, 0.30588235],\n",
       "         [0.32156863, 0.23137255, 0.30196078],\n",
       "         [0.3254902 , 0.21568627, 0.29803922]]],\n",
       "\n",
       "\n",
       "       [[[0.10196078, 0.03921569, 0.08235294],\n",
       "         [0.09019608, 0.03529412, 0.07843137],\n",
       "         [0.10980392, 0.03921569, 0.08235294],\n",
       "         ...,\n",
       "         [0.10588235, 0.03137255, 0.07843137],\n",
       "         [0.09411765, 0.01568627, 0.0745098 ],\n",
       "         [0.08627451, 0.03137255, 0.0745098 ]],\n",
       "\n",
       "        [[0.10980392, 0.03137255, 0.07843137],\n",
       "         [0.10588235, 0.03921569, 0.08235294],\n",
       "         [0.10588235, 0.04313725, 0.08235294],\n",
       "         ...,\n",
       "         [0.10980392, 0.03529412, 0.07843137],\n",
       "         [0.09019608, 0.02352941, 0.07843137],\n",
       "         [0.08627451, 0.01568627, 0.0745098 ]],\n",
       "\n",
       "        [[0.10588235, 0.03921569, 0.08235294],\n",
       "         [0.10980392, 0.03921569, 0.08235294],\n",
       "         [0.10588235, 0.04705882, 0.08235294],\n",
       "         ...,\n",
       "         [0.10588235, 0.03529412, 0.08235294],\n",
       "         [0.09411765, 0.02352941, 0.07843137],\n",
       "         [0.09411765, 0.01960784, 0.07843137]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.08627451, 0.03529412, 0.08235294],\n",
       "         [0.11372549, 0.03529412, 0.08235294],\n",
       "         [0.10980392, 0.0627451 , 0.08235294],\n",
       "         ...,\n",
       "         [0.10196078, 0.02745098, 0.07843137],\n",
       "         [0.09411765, 0.02745098, 0.07843137],\n",
       "         [0.09019608, 0.01568627, 0.07843137]],\n",
       "\n",
       "        [[0.10588235, 0.03529412, 0.08235294],\n",
       "         [0.10196078, 0.02352941, 0.08235294],\n",
       "         [0.10588235, 0.03529412, 0.08235294],\n",
       "         ...,\n",
       "         [0.10588235, 0.02352941, 0.07843137],\n",
       "         [0.09411765, 0.02352941, 0.07843137],\n",
       "         [0.09803922, 0.03137255, 0.0745098 ]],\n",
       "\n",
       "        [[0.09411765, 0.03921569, 0.08235294],\n",
       "         [0.10588235, 0.05490196, 0.08235294],\n",
       "         [0.10980392, 0.03921569, 0.08235294],\n",
       "         ...,\n",
       "         [0.09803922, 0.02745098, 0.0745098 ],\n",
       "         [0.09803922, 0.03137255, 0.0745098 ],\n",
       "         [0.09803922, 0.01960784, 0.0745098 ]]],\n",
       "\n",
       "\n",
       "       [[[0.14509804, 0.06666667, 0.08627451],\n",
       "         [0.15294118, 0.09411765, 0.08627451],\n",
       "         [0.15686275, 0.07058824, 0.08627451],\n",
       "         ...,\n",
       "         [0.12156863, 0.05882353, 0.08235294],\n",
       "         [0.13333333, 0.05098039, 0.08235294],\n",
       "         [0.11764706, 0.05098039, 0.08235294]],\n",
       "\n",
       "        [[0.15294118, 0.0627451 , 0.08235294],\n",
       "         [0.14901961, 0.0745098 , 0.09411765],\n",
       "         [0.15294118, 0.07058824, 0.09019608],\n",
       "         ...,\n",
       "         [0.12156863, 0.03921569, 0.07843137],\n",
       "         [0.10588235, 0.04313725, 0.08627451],\n",
       "         [0.14901961, 0.05490196, 0.08235294]],\n",
       "\n",
       "        [[0.15294118, 0.0745098 , 0.09019608],\n",
       "         [0.15294118, 0.0745098 , 0.09803922],\n",
       "         [0.14901961, 0.07058824, 0.09019608],\n",
       "         ...,\n",
       "         [0.10980392, 0.03921569, 0.08235294],\n",
       "         [0.13333333, 0.05098039, 0.08235294],\n",
       "         [0.1372549 , 0.05098039, 0.08235294]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.17647059, 0.09019608, 0.08627451],\n",
       "         [0.16078431, 0.0745098 , 0.09411765],\n",
       "         [0.14901961, 0.06666667, 0.09411765],\n",
       "         ...,\n",
       "         [0.11372549, 0.04705882, 0.08235294],\n",
       "         [0.12156863, 0.05098039, 0.08235294],\n",
       "         [0.14901961, 0.04705882, 0.08235294]],\n",
       "\n",
       "        [[0.16470588, 0.07058824, 0.09019608],\n",
       "         [0.14901961, 0.07843137, 0.09411765],\n",
       "         [0.15294118, 0.08627451, 0.09019608],\n",
       "         ...,\n",
       "         [0.14901961, 0.04705882, 0.08627451],\n",
       "         [0.15294118, 0.05098039, 0.08235294],\n",
       "         [0.11372549, 0.05490196, 0.08235294]],\n",
       "\n",
       "        [[0.15294118, 0.06666667, 0.09019608],\n",
       "         [0.15294118, 0.0745098 , 0.08627451],\n",
       "         [0.14901961, 0.05098039, 0.09019608],\n",
       "         ...,\n",
       "         [0.15294118, 0.06666667, 0.08627451],\n",
       "         [0.15686275, 0.0627451 , 0.08235294],\n",
       "         [0.15294118, 0.0627451 , 0.08235294]]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_train = np.array(x_train)\n",
    "x_train = np.array(x_train)/255\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7689, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y_train.reshape(-1,1)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images//K-039136\\K-039136_0_1_0_0_70_180_200.png\n",
      "images//K-038958\\K-038958_0_2_0_1_90_000_200.png\n",
      "images//K-039021\\K-039021_0_2_0_2_90_040_200.png\n",
      "images//K-039147\\K-039147_0_0_0_1_75_160_200.png\n",
      "images//K-038896\\K-038896_0_1_0_2_90_120_200.png\n",
      "images//K-038910\\K-038910_0_0_1_0_70_100_200.png\n",
      "images//K-039136\\K-039136_0_1_0_2_60_120_200.png\n",
      "images//K-038970\\K-038970_0_2_0_0_90_100_200.png\n",
      "images//K-038927\\K-038927_0_0_1_2_90_080_200.png\n",
      "images//K-038912\\K-038912_0_0_0_2_75_140_200.png\n",
      "images//K-039147\\K-039147_0_2_1_1_70_260_200.png\n",
      "images//K-039147\\K-039147_0_2_1_0_60_000_200.png\n",
      "images//K-039136\\K-039136_0_2_0_1_70_020_200.png\n",
      "images//K-039108\\K-039108_0_0_0_0_90_240_200.png\n",
      "images//K-038914\\K-038914_0_0_0_0_90_120_200.png\n",
      "images//K-039146\\K-039146_0_1_0_1_70_080_200.png\n",
      "images//K-039047\\K-039047_0_2_1_0_90_240_200.png\n",
      "images//K-038913\\K-038913_0_0_0_0_75_240_200.png\n",
      "images//K-039147\\K-039147_0_2_1_1_60_060_200.png\n",
      "images//K-039021\\K-039021_0_2_1_2_90_240_200.png\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = [], []\n",
    "\n",
    "for idx, x in enumerate(x_test_names):\n",
    "  if idx % 100 == 0:\n",
    "    print(x)\n",
    "  img = cv2.cvtColor(cv2.imread(x, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "  id = x[s:s+8]\n",
    "  temp_cls = cls[id]\n",
    "  y_test.append(temp_cls)\n",
    "  x_test.append(img)\n",
    "\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "# for idx, x in enumerate(x_test_names):\n",
    "#   if idx % 100 == 0:\n",
    "#     print('{} / {}'.format(idx, len(x_test_names)))\n",
    "#   f = tf.io.read_file(x)\n",
    "#   img = tf.io.decode_image(f)\n",
    "#   x_test.append(img)\n",
    "#   id = x[s:s+8]\n",
    "#   temp_cls = cls[id]\n",
    "#   y_test.append(temp_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test/255\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1923, 64, 64, 3), (1923, 1))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                98328     \n",
      "=================================================================\n",
      "Total params: 173,976\n",
      "Trainable params: 173,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "# con 1\n",
    "model.add(tf.keras.layers.Conv2D(64,3,padding='same',activation='relu',input_shape=(64,64,3)))  # conv2d 수행하면 64채널의 결과가 나오고, kernelsize = 3\n",
    "model.add(tf.keras.layers.Dropout(rate=0.5))                          #  DO1\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "# conv 2\n",
    "model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(rate=0.5))                          # DO2\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "# conv 3\n",
    "model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(rate=0.5))                           # DO3\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "# dense layers\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(len(cls), activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "               loss='sparse_categorical_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = './drug.h5'\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10,restore_best_weights=True)\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(saved_model, monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "241/241 [==============================] - 11s 15ms/step - loss: 1.0552 - accuracy: 0.6326 - val_loss: 1.5923 - val_accuracy: 0.7525\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.75247, saving model to .\\drug.h5\n",
      "Epoch 2/100\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.4531 - accuracy: 0.8102 - val_loss: 1.3731 - val_accuracy: 0.7166\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.75247\n",
      "Epoch 3/100\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.3844 - accuracy: 0.8251 - val_loss: 1.4786 - val_accuracy: 0.5949\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.75247\n",
      "Epoch 4/100\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.3556 - accuracy: 0.8376 - val_loss: 1.2281 - val_accuracy: 0.8149\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.75247 to 0.81487, saving model to .\\drug.h5\n",
      "Epoch 5/100\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.3508 - accuracy: 0.8439 - val_loss: 1.1376 - val_accuracy: 0.7457\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.81487\n",
      "Epoch 6/100\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.3235 - accuracy: 0.8564 - val_loss: 1.1427 - val_accuracy: 0.7249\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.81487\n",
      "Epoch 7/100\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.3579 - accuracy: 0.8439 - val_loss: 0.9773 - val_accuracy: 0.6953\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.81487\n",
      "Epoch 8/100\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.3003 - accuracy: 0.8614 - val_loss: 1.0271 - val_accuracy: 0.7551\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.81487\n",
      "Epoch 9/100\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.2827 - accuracy: 0.8697 - val_loss: 0.9385 - val_accuracy: 0.7702\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.81487\n",
      "Epoch 10/100\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.2765 - accuracy: 0.8740 - val_loss: 0.8868 - val_accuracy: 0.7624\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.81487\n",
      "Epoch 11/100\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.3086 - accuracy: 0.8623 - val_loss: 0.8440 - val_accuracy: 0.7296\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.81487\n",
      "Epoch 12/100\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.2829 - accuracy: 0.8710 - val_loss: 0.9227 - val_accuracy: 0.7977\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.81487\n",
      "Epoch 13/100\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.2736 - accuracy: 0.8738 - val_loss: 0.7053 - val_accuracy: 0.8071\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.81487\n",
      "Epoch 14/100\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.2625 - accuracy: 0.8829 - val_loss: 0.8069 - val_accuracy: 0.7629\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.81487\n",
      "Epoch 15/100\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.2581 - accuracy: 0.8846 - val_loss: 0.7261 - val_accuracy: 0.8476\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.81487 to 0.84763, saving model to .\\drug.h5\n",
      "Epoch 16/100\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.2602 - accuracy: 0.8798 - val_loss: 0.8250 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.84763\n",
      "Epoch 17/100\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.2478 - accuracy: 0.8895 - val_loss: 0.8334 - val_accuracy: 0.7254\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.84763\n",
      "Epoch 18/100\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.2440 - accuracy: 0.8889 - val_loss: 1.0112 - val_accuracy: 0.6843\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.84763\n",
      "Epoch 19/100\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.2472 - accuracy: 0.8883 - val_loss: 0.7349 - val_accuracy: 0.7410\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.84763\n",
      "Epoch 20/100\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.2350 - accuracy: 0.8947 - val_loss: 0.7163 - val_accuracy: 0.7592\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.84763\n",
      "Epoch 21/100\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.2499 - accuracy: 0.8869 - val_loss: 0.9223 - val_accuracy: 0.7119\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.84763\n",
      "Epoch 22/100\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.2374 - accuracy: 0.8978 - val_loss: 0.9341 - val_accuracy: 0.6693\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.84763\n",
      "Epoch 23/100\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.2383 - accuracy: 0.8989 - val_loss: 0.9686 - val_accuracy: 0.7431\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.84763\n",
      "Epoch 00023: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "          epochs = 100,\n",
    "          callbacks=[es, mc],\n",
    "          batch_size = 32,\n",
    "          validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 모델을 통해 84% 정도의 정확도로 구별이 가능하다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                393240    \n",
      "=================================================================\n",
      "Total params: 764,056\n",
      "Trainable params: 764,056\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = tf.keras.models.Sequential()\n",
    "#\n",
    "model1.add(tf.keras.layers.Conv2D(64,3,padding='same',activation='relu',input_shape=(64,64,3)))  # conv2d 수행하면 64채널의 결과가 나오고, kernelsize = 3\n",
    "model1.add(tf.keras.layers.Dropout(rate=0.5))                          #  DO1\n",
    "model1.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "#\n",
    "model1.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "model1.add(tf.keras.layers.Dropout(rate=0.5))                          # DO2\n",
    "model1.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "#\n",
    "model1.add(tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'))\n",
    "model1.add(tf.keras.layers.Dropout(rate=0.5))                           # DO3\n",
    "model1.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "#\n",
    "model1.add(tf.keras.layers.Flatten())\n",
    "model1.add(tf.keras.layers.Dense(len(cls), activation='softmax'))\n",
    "#\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam',\n",
    "               loss='sparse_categorical_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "241/241 [==============================] - 5s 17ms/step - loss: 1.0324 - accuracy: 0.6370 - val_loss: 1.5050 - val_accuracy: 0.5757\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.84763\n",
      "Epoch 2/100\n",
      "241/241 [==============================] - 4s 16ms/step - loss: 0.4379 - accuracy: 0.8134 - val_loss: 1.2812 - val_accuracy: 0.7379\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.84763\n",
      "Epoch 3/100\n",
      "241/241 [==============================] - 4s 16ms/step - loss: 0.3833 - accuracy: 0.8322 - val_loss: 1.3049 - val_accuracy: 0.5907\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.84763\n",
      "Epoch 4/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.3322 - accuracy: 0.8503 - val_loss: 1.2923 - val_accuracy: 0.6323\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.84763\n",
      "Epoch 5/100\n",
      "241/241 [==============================] - 4s 16ms/step - loss: 0.3232 - accuracy: 0.8556 - val_loss: 1.0882 - val_accuracy: 0.6687\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.84763\n",
      "Epoch 6/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.3172 - accuracy: 0.8604 - val_loss: 1.1460 - val_accuracy: 0.6734\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.84763\n",
      "Epoch 7/100\n",
      "241/241 [==============================] - 4s 16ms/step - loss: 0.2791 - accuracy: 0.8758 - val_loss: 0.8574 - val_accuracy: 0.7561\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.84763\n",
      "Epoch 8/100\n",
      "241/241 [==============================] - 4s 16ms/step - loss: 0.2922 - accuracy: 0.8705 - val_loss: 1.1859 - val_accuracy: 0.5887\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.84763\n",
      "Epoch 9/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.2902 - accuracy: 0.8738 - val_loss: 0.8615 - val_accuracy: 0.6859\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.84763\n",
      "Epoch 10/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.2676 - accuracy: 0.8824 - val_loss: 0.7367 - val_accuracy: 0.7639\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.84763\n",
      "Epoch 11/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.2417 - accuracy: 0.8923 - val_loss: 0.6661 - val_accuracy: 0.8190\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.84763\n",
      "Epoch 12/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.2402 - accuracy: 0.8961 - val_loss: 0.9348 - val_accuracy: 0.7088\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.84763\n",
      "Epoch 13/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.2298 - accuracy: 0.8969 - val_loss: 0.9042 - val_accuracy: 0.6901\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.84763\n",
      "Epoch 14/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.2399 - accuracy: 0.8982 - val_loss: 0.7753 - val_accuracy: 0.7254\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.84763\n",
      "Epoch 15/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.2353 - accuracy: 0.8971 - val_loss: 1.0410 - val_accuracy: 0.6625\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.84763\n",
      "Epoch 16/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.2162 - accuracy: 0.9075 - val_loss: 0.5671 - val_accuracy: 0.8612\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.84763 to 0.86115, saving model to .\\drug.h5\n",
      "Epoch 17/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.2369 - accuracy: 0.9028 - val_loss: 1.1997 - val_accuracy: 0.6147\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.86115\n",
      "Epoch 18/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.2092 - accuracy: 0.9131 - val_loss: 1.0247 - val_accuracy: 0.6422\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.86115\n",
      "Epoch 19/100\n",
      "241/241 [==============================] - 4s 16ms/step - loss: 0.2014 - accuracy: 0.9112 - val_loss: 0.8345 - val_accuracy: 0.7015\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.86115\n",
      "Epoch 20/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.1889 - accuracy: 0.9188 - val_loss: 0.9574 - val_accuracy: 0.6682\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.86115\n",
      "Epoch 21/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.1990 - accuracy: 0.9160 - val_loss: 0.6070 - val_accuracy: 0.8086\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.86115\n",
      "Epoch 22/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.2254 - accuracy: 0.9112 - val_loss: 0.6286 - val_accuracy: 0.7977\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.86115\n",
      "Epoch 23/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.1948 - accuracy: 0.9165 - val_loss: 0.4869 - val_accuracy: 0.8528\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.86115\n",
      "Epoch 24/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.1783 - accuracy: 0.9240 - val_loss: 1.2259 - val_accuracy: 0.6037\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.86115\n",
      "Epoch 25/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.1697 - accuracy: 0.9313 - val_loss: 0.9121 - val_accuracy: 0.7124\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.86115\n",
      "Epoch 26/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.1623 - accuracy: 0.9338 - val_loss: 1.5250 - val_accuracy: 0.5741\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.86115\n",
      "Epoch 27/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.1542 - accuracy: 0.9378 - val_loss: 0.8173 - val_accuracy: 0.6994\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.86115\n",
      "Epoch 28/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.1666 - accuracy: 0.9332 - val_loss: 1.4426 - val_accuracy: 0.5996\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.86115\n",
      "Epoch 29/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.1557 - accuracy: 0.9382 - val_loss: 1.1813 - val_accuracy: 0.6386\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.86115\n",
      "Epoch 30/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.1703 - accuracy: 0.9303 - val_loss: 1.1304 - val_accuracy: 0.6271\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.86115\n",
      "Epoch 31/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.1601 - accuracy: 0.9365 - val_loss: 1.3418 - val_accuracy: 0.6245\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.86115\n",
      "Epoch 32/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.1434 - accuracy: 0.9415 - val_loss: 1.1996 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.86115\n",
      "Epoch 33/100\n",
      "241/241 [==============================] - 4s 15ms/step - loss: 0.1325 - accuracy: 0.9471 - val_loss: 0.9006 - val_accuracy: 0.7478\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.86115\n",
      "Epoch 00033: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(x_train, y_train,\n",
    "          epochs = 100,\n",
    "          callbacks=[es, mc],\n",
    "          batch_size = 32,\n",
    "          validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chaddol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a687caacb10a844a55fefd3b5f8dd537474e79b4cd9a59a4e226ce1d72456a88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
